@multiarg futon0/devmap
@title FUTON0 Development Map â€” Human-System Interface
@audience futon-devs, reflective-agents, future-you
@tone formal-analytic
@style roadmap
@factor Mindful apprehension (sati + sampajaÃ±Ã±a)
@IFR: FUTON0 is the interface through which the system extends human capability.
It monitors vitality, rhythm, and attention, making human state legible to the
stack and stack state perceivable to the human. The human is not a component of
the system; the system is an extension of the human.
@state P0 (core infrastructure) operational: Stack HUD, bidirectional channels,
infrastructure substrate working. P1-P7 (layer interfaces) not yet established.
@next Establish F0-F1 interface (memory access), then F0-F2 (agent perception).

## The Argument

FUTON0 is the interface through which the system extends human capability to
reason, remember, verify, compose, and act effectively. The Stack HUD, voice
channels, and infrastructure substrate establish that the interface works at all
(P0). The remaining work is to extend this interface across each layer of the
stack: F1 (memory), F2 (agents), F3 (verification), F4 (hypertext), F5
(patterns), F6 (mathematics), F7 (transparency). When all layer interfaces work,
F0 is complete â€” the human can access the full capability of the system through
a unified interface. Each layer reflects the whole from its perspective; F0 sees
vitality, rhythm, and attention as the ground from which all other capabilities
become accessible.


! instantiated-by: Prototype 0 â€” Core Infrastructure [ğŸ™‡/äº ğŸ”ƒ/å‰]
  :maturity :active
  :depends-on []
  :evidence-for-active [Stack HUD operational; bidirectional channels demonstrated; infrastructure working]

  + context: The interface must work at all before it can extend human capability.
  + IF: The human needs to perceive stack state and the stack needs to perceive human vitality
  + HOWEVER: These capabilities require working infrastructure, channels, and monitoring
  + THEN: Establish Stack HUD, bidirectional channels (voice, keyboard, TTS), infrastructure substrate, health monitoring, affect signals, epistemic rhythm, and stewardship practices
  + BECAUSE: This is the foundation â€” without it, no layer interface can function

  + EVIDENCE:
    evidence[Stack HUD (futon0/contrib/stack-hud.el) displays vitality + stack state]
    evidence[Voice-to-text, text-to-speech, Kinesis board operational]
    evidence[Infrastructure substrate (Alacritty, Emacs, Linode, FUTON1, Codex, Claude) working]
    evidence[Vitality scanner captures commit times and session timestamps]

  + NEXT-STEPS:
    next[Complete health monitoring (sleep inference, energy inference)]
    next[Complete affect signal extraction from logged sessions]
    next[Complete epistemic rhythm checks (weekly/monthly)]
    next[Document stewardship protocol]

  :success-criteria
    pass[HUD displays vitality + stack state within 2 seconds]
    pass[Bidirectional channels feel unified, not siloed]
    pass[Health/affect/rhythm signals are meaningful, not noise]
    fail[Core infrastructure requires constant manual intervention]


! instantiated-by: Prototype 1 â€” F0-F1 Interface (Memory Access) [ğŸ“/å¿† ğŸ‘“/è§]
  :maturity :stub
  :depends-on [f0/P0, f1/P0]

  + context: F1 is graph memory â€” fragments, relationships, annotations as nodes.
  + IF: The human needs to access, query, and navigate stored knowledge
  + HOWEVER: Currently F1 access requires direct API calls or Emacs functions
  + THEN: Surface memory access through F0 â€” query from HUD, see relationships, navigate links
  + BECAUSE: If the human cannot easily access memory, F1 does not extend their capability

  + NEXT-STEPS:
    next[Add memory query widget to Stack HUD]
    next[Surface recent fragments and their relationships]
    next[Enable link navigation from HUD context]

  :success-criteria
    pass[Human can query F1 without leaving the HUD context]
    pass[Relationships are visible and navigable]
    fail[F1 access remains a separate, friction-full operation]


! instantiated-by: Prototype 2 â€” F0-F2 Interface (Agent Perception) [ğŸ“/æ‰ ğŸ‘€/è§]
  :maturity :stub
  :depends-on [f0/P0, f2/P0]

  + context: F2 is active inference â€” perception-action loops for agents.
  + IF: The human needs to perceive what agents are doing and direct their actions
  + HOWEVER: Currently agent sessions are logged but not surfaced in real-time
  + THEN: Surface agent state through F0 â€” see current beliefs, policies, actions; redirect when needed
  + BECAUSE: If the human cannot perceive agents, they cannot effectively collaborate with them

  + NEXT-STEPS:
    next[Surface active agent sessions in Stack HUD]
    next[Show agent beliefs and current policy]
    next[Enable human redirection of agent focus]

  :success-criteria
    pass[Human can see what agents are doing without reading logs]
    pass[Human can redirect agents through F0 interface]
    fail[Agent state remains opaque until session ends]


! instantiated-by: Prototype 3 â€” F0-F3 Interface (Verification Visibility) [ğŸ˜„/æ­£ ğŸ“/æ­£]
  :maturity :stub
  :depends-on [f0/P0, f3/P0]

  + context: F3 is pattern checking â€” validation of informal arguments.
  + IF: The human needs to see verification status of claims and arguments
  + HOWEVER: Currently checks are logged but not surfaced to F0
  + THEN: Surface verification status through F0 â€” see which claims are checked, what evidence exists, what is missing
  + BECAUSE: If the human cannot see verification status, they cannot trust or improve arguments

  + NEXT-STEPS:
    next[Surface recent checks in Stack HUD]
    next[Show evidence status for active claims]
    next[Highlight missing evidence or blocked checks]

  :success-criteria
    pass[Human can see verification status at a glance]
    pass[Missing evidence is visible and actionable]
    fail[Verification remains invisible until explicitly queried]


! instantiated-by: Prototype 4 â€” F0-F4 Interface (Hypertext Navigation) [ğŸ“/æ–‡ â°/ä¹…]
  :maturity :stub
  :depends-on [f0/P0, f4/P0]

  + context: F4 is hypertext editing â€” documents as graph nodes with relationships.
  + IF: The human needs to navigate documents and their relationships
  + HOWEVER: Currently document navigation is through Emacs/Arxana directly
  + THEN: Surface document structure through F0 â€” see current document context, related documents, annotations
  + BECAUSE: If the human cannot navigate hypertext fluidly, document relationships remain implicit

  + NEXT-STEPS:
    next[Surface document context in Stack HUD]
    next[Show related documents and annotations]
    next[Enable quick navigation to linked fragments]

  :success-criteria
    pass[Human can see document relationships from HUD]
    pass[Navigation between related documents is fluid]
    fail[Document relationships require manual search]


! instantiated-by: Prototype 5 â€” F0-F5 Interface (Pattern Work) [ğŸ’/æœ¬ ğŸ”ƒ/å»]
  :maturity :stub
  :depends-on [f0/P0, f5/P0]

  + context: F5 is wiring diagrams â€” compositional pattern transfer across domains.
  + IF: The human needs to work with patterns, see transfers, compose diagrams
  + HOWEVER: Currently pattern work requires understanding category theory notation
  + THEN: Surface pattern structure through F0 â€” see active patterns, visualize wiring, understand transfers
  + BECAUSE: If the human cannot perceive patterns, they cannot leverage compositional abstraction

  + NEXT-STEPS:
    next[Surface active patterns and their domains in Stack HUD]
    next[Visualize wiring diagrams in accessible form]
    next[Show pattern transfers between domains]

  :success-criteria
    pass[Human can see which patterns are active and where]
    pass[Wiring diagrams are comprehensible without CT background]
    fail[Pattern work remains expert-only]


! instantiated-by: Prototype 6 â€” F0-F6 Interface (Mathematical Access) [âœŒï¸/è®¡ ğŸ“/ä¹¦]
  :maturity :stub
  :depends-on [f0/P0, f6/P0]

  + context: F6 is mathematical dictionary â€” definitions, proofs, examples with cross-references.
  + IF: The human needs random access to mathematical knowledge
  + HOWEVER: Currently math access is through linear reading or search
  + THEN: Surface mathematical structure through F0 â€” query definitions, see dependencies, navigate proofs
  + BECAUSE: If the human cannot access math nonlinearly, the dictionary structure provides no advantage

  + NEXT-STEPS:
    next[Add math query capability to Stack HUD]
    next[Surface definition dependencies and examples]
    next[Enable proof navigation from HUD context]

  :success-criteria
    pass[Human can query mathematical concepts from HUD]
    pass[Dependencies between definitions are visible]
    fail[Math access remains search-based with no structure]


! instantiated-by: Prototype 7 â€” F0-F7 Interface (Transparency/Simulation) [ğŸ‘“/å…¥ ğŸ’–/æœ¬]
  :maturity :stub
  :depends-on [f0/P0, f7/P0]

  + context: F7 is transparency and civic simulation â€” annotations on web content, scenario modeling.
  + IF: The human needs to see transparency annotations and run simulations
  + HOWEVER: Currently these are separate browser extensions and notebooks
  + THEN: Surface transparency and simulation through F0 â€” see annotations, run scenarios, understand models
  + BECAUSE: If the human cannot access transparency tools fluidly, economic structures remain opaque

  + NEXT-STEPS:
    next[Surface active annotations in Stack HUD]
    next[Enable scenario simulation from HUD context]
    next[Show model assumptions and results]

  :success-criteria
    pass[Human can see transparency annotations from HUD]
    pass[Simulations are runnable without context switch]
    fail[Transparency tools remain siloed from main interface]


! instantiated-by: Completion â€” Full Layer Integration [ğŸŒ½/æœ« ğŸŒ/ä¸‡]
  :maturity :greenfield
  :depends-on [f0/P0, f0/P1, f0/P2, f0/P3, f0/P4, f0/P5, f0/P6, f0/P7]

  + context: F0 is complete when all layer interfaces work.
  + IF: P0-P7 are all operational
  + HOWEVER: Integration may reveal gaps not visible in individual prototypes
  + THEN: Validate that the human can access all stack capabilities through F0
  + BECAUSE: The system extends human capability only if all layers are accessible

  + NEXT-STEPS:
    next[Integration test: access all layers through morning review workflow]
    next[Identify friction points in layer transitions]
    next[Validate that capability extension is real, not theoretical]

  :success-criteria
    pass[Human can access memory, agents, verification, hypertext, patterns, math, transparency through unified interface]
    pass[Layer transitions are fluid, not jarring]
    fail[Some layers remain inaccessible or require context switch]
