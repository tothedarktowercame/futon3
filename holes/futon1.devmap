@multiarg futon1/devmap
@title FUTON1 Development Map
@audience futon-devs, infra-agents, future-you
@tone formal-analytic
@style roadmap
@allow-new-claims true
@length any
@factor Keen investigation (dhammavicaya)
@IFR: FUTON1 is the dhammavicaya layer: a deterministic, well-documented stack where chat demos, NLP, graph memory, and XTDB stay locked in sync so every focus header, ingest script, and CLI transcript has a reproducible twin for higher futonsâ€”turning the act of building the fact graph into keen investigation.
@state Operational stack shipping focus-header demos; documentation/tests drift around NLP interface and XT instrumentation still unresolved.
@next Refresh focus-header scripts, consolidate nlp-interface documentation/tests, and publish the XT operations stories promised by Prototype 4.


! instantiated-by: Prototype 0 â€” Baseline Stack Snapshot [ğŸ’¤/å·² â›²ï¸/å¸ƒ]
  + context: futon1 is a collection of deterministic chat demos built on a tiered NLP stack, in-memory graph-memory, and XTDB persistence, surfaced via `apps/basic-chat-demo`, `apps/nlp-interface`, `apps/graph-memory`, and `apps/open-world-ingest`.
  + if: You want a clear picture of what already works before you add new affordances.
  + however: The current documentation is spread across multiple READMEs and doesnâ€™t yet tell a single â€œstack storyâ€ from CLI to XTDB.
  + then: Freeze the current behaviour with tags, record a concise architecture note (pipeline â†’ graph â†’ XT), and capture representative CLI transcripts for v1â€“v5.
  + because: This establishes a stable reference snapshot and makes regressions and future extensions legible.

! instantiated-by: Prototype 1 â€” basic-chat/v5 Hardening (Focus Header Path) [ğŸ’¤/æ¯ ğŸš/å·²]
  + context: the `demo` app is the canonical way to interrogate the stack; it emits focus headers, records hydration, and demonstrates how NLP, graph-memory, and XTDB cohere in practice.
  + if: You want the CLI to behave like calibrated instrumentation instead of a friendly-but-shaky demo.
  + however: our current smoke tests hit only the happy path, leaving hydration races, RocksDB locks, and legacy protocol quirks to intuition.
  + then: design an automated focus-header lab: scripted sessions that flip every flag, replay historical transcripts, simulate flaky transports, and publish the resulting traces alongside expected XTDB states.
  + because: treating `basic-chat` as calibrated instrumentation (not a toy) keeps every downstream agent honest about what the transport and memory layers actually guarantee.

! instantiated-by: Prototype 2 â€” NLP Interface Consolidation [ğŸ‘´/ç”³ ğŸ“/è¿™äº›]
  + context: `apps/nlp-interface` provides the deterministic NER/POS and intent layers shared across protocols, including v4 gazetteer and pattern recogniser hooks.
  + if: You want a single, testable entry point for the NLP stack that can evolve independently of the chat CLI.
  + however: The nlp-interface README is still boilerplate and doesnâ€™t advertise the actual pipeline or RID-style intent dictionary.
  + then: Refactor nlp-interface into explicit stages (tokenise â†’ tag â†’ chunk â†’ intent), add unit tests for each, and write a focused README for these APIs.
  + because: A consolidated NLP module makes it easier to swap or extend components without destabilising the demos.

! instantiated-by: Prototype 3 â€” Graph-Memory API & Seed Graph [ğŸ’¤/å·² ğŸ‘/ä¹ˆ]
  + context: `graph-memory` already encodes the Datascript schema, salience metadata, and helper fns that everything else relies onâ€”but only insiders know how to call it.
  + if: You want futon1 to expose a principled contract so other futons (and external agents) can trust the graph.
  + however: helper fns are tuned for REPL spelunking, naming is inconsistent, and there is no formal description of what gets mirrored into XTDB.
  + then: promote `graph-memory` to a documented API: name the seed graph, publish init/upsert/link/export specs, generate API docs straight from tests, and treat breaking changes like migrations.
  + because: a graph-memory contractâ€”rather than comfy REPL folkloreâ€”is what lets FUTON3 proofs and FUTON7 civic tooling depend on the same facts.
  + next steps: DONE?: check the that release candidate matches this spec (futon1/apps/graph-memory/README.md:1-160); DOCS: do an *audit* of currently generated content to check the workflow

! instantiated-by: Prototype 4 â€” XTDB Operations & REPL Stories [ğŸœ/åˆ† ğŸº/ç”²]
  + context: The stack mirrors all graph mutations into XTDB and provides notes for running a REPL against the same datastore.
  + if: You want developers to comfortably inspect, query, and repair the persistent store.
  + however: Common workflows (inspection, counting, ego traversal, cleanup) still require reading raw XT queries and ad-hoc snippets.
  + then: Package REPL recipes as reusable fns (`list-entities`, `entity-relations`, `counts`), add a small â€œXT storiesâ€ doc with copy-paste examples, and script a salience-survives-restart demo.
  + because: Lowering XT friction makes the persistent layer feel like a first-class part of futon1 rather than a hidden backend.
  + evidence: âœ… README-storage.md plus the :storage/inspect entry point document/automate the inspection workflow
  + next: surface the same schema view in Emacs so futon3 can browse XT contents without guessing paths.

! instantiated-by: Prototype 5 â€” Open-World Ingest Workflows [ğŸ”¦/ä¸–ç•Œ ğŸ²/å¼€]
  + context: The open-world ingest tool streams arbitrary text through CoreNLP, generating deterministic entity IDs and OpenIE relations in XTDB.
  + if: You want futon1 to support offline bulk ingestion as well as interactive chat.
  + however: Ingest currently lives as a standalone CLI with minimal guidance on how its data joins the basic-chat focus-header world.
  + then: Define standard data-dir layouts for shared XTDB, write small pipelines that ingest corpora then query them from `basic-chat-demo`, and add golden tests for ingest semantics (ID stability, negation flags, co-occurrence).
  + because: Robust ingest workflows let futon1 accumulate long-term background knowledge, not just per-session memories.

! instantiated-by: Prototype 6 â€” Intent Dictionary & Thematic Clusters [ğŸ™…/æ—  ğŸ’¤/ä¸Š]
  + context: The deterministic intent classifier already maps utterances onto conversational and RID-inspired clusters, but today those labels peak inside the logs.
  + if: You want intent to act as policyâ€”steering focus headers, salience boosts, and downstream pattern choices.
  + however: there is no canonical intent taxonomy, no hooks from classifier output into the CLI/graph, and no regression suites proving these effects.
  + then: publish the intent dictionary as a living document, wire intents into the focus-header builder (boost/suppress entities, tag trails), and add fixtures showing how each theme alters XTDB mirrors and futon3 checks.
  + because: intent only matters if it shapes behaviour; otherwise the classifier is cosplay rather than dhammavicaya.

! instantiated-by: Prototype 7 â€” CLI Ergonomics & Multi-Instance Isolation [ğŸ/èµ„æ–™ ğŸ’/ç¤º]
  + context: The CLI already exposes bang commands, context flags, and environment variables for data-dir and XT config overrides.
  + if: You want futon1 to run multiple isolated demos (tests, experiments, production-like runs) on the same host.
  + however: Best practices for `BASIC_CHAT_DATA_DIR`, XT config selection, and `--reset/--compact/--export` are scattered and easy to misuse.
  + then: Ship a â€œprofilesâ€ doc (dev/test/prod) with recommended flags and env vars, add smoke tests that spin up multiple instances with different data-dirs, and provide a `/status` helper to show active config.
  + because: Good ergonomics and isolation make futon1 safer to embed in other tools and workflows.

! instantiated-by: Prototype 8 â€” Golden Scripts & Performance Benchmarks [ğŸœ/ä¸è¦ ğŸŒ±/ä¹…]
  + context: Scripted EDN conversations and golden outputs already exercise the focus-header flow and older basic-chat protocols.
  + if: You want confidence that future changes donâ€™t break determinism, latency thresholds, or header quality.
  + however: Golden coverage is still patchy and doesnâ€™t include dedicated performance baselines or large-script runs.
  + then: Curate a small suite of canonical scripts (short-chat, long-history, ingest+chat), time them under CI, and treat both outputs and runtime ranges as golden.
  + because: Benchmarks turn futon1 into a measurable platform rather than an anecdotal demo.

! instantiated-by: Prototype 9 â€” External Client & Agent Bridge [ğŸ“/æ—  ğŸ™…/å·²]
  + context: Focus-header EDN is already consumable by chatgpt-shell, Codex, or futon4, but every integration has been bespoke.
  + if: You want futon1 to expose a purposeful bridge so external agents see the same contract as the CLI.
  + however: there is no signed JSON schema, no HTTP shim, and no reference client showing how to stream utterances and receive headers deterministically.
  + then: publish a tiny `/focus` API, wrap the CLI in a documented RPC, and ship sample clients (Clojure + at least one other language) that replay real transcripts against it.
  + because: when the bridge is first-class, you can swap or stack agents freely without forking the core or relying on brittle stdout scraping.

! instantiated-by: Prototype 10 â€” Futon4 / Arxana Export Hooks [ğŸ’–/å·² ğŸ’¤/æœ¬]
  + context: futon4 will want to ingest focus headers, EDN snapshots, and XTDB slices as graph inputs for Arxana and higher-level pattern work.
  + if: You want futon1 to be the canonical source of â€œworld stateâ€ for the year-long prototype cycle.
  + however: There is no named export profile tailored for Arxana or datascript/XT interop beyond generic `--export edn`.
  + then: Add an â€œarxana-exportâ€ mode that emits a compressed, schema-tagged snapshot of entities, relations, and salience suitable for futon4 import.
  + because: Tight, named integration turns futon1 into the lower memory layer of the FUTON stack rather than a sibling demo.

! instantiated-by: Prototype 11 â€” AI Scientist Pilot Scenarios [ğŸ‘­/æ‰]
  + context: The AI Scientist pilots exercised futon1/futon3 side-by-sideâ€”joyful reasoning loops that leaned on focus headers, ingest, and proof logging.
  + if: You want those pilots to serve as canonical fixtures anyone can replay or extend.
  + however: the transcripts, CLI flags, and seed graphs live in private folders, so nobody can cite or re-run them with confidence.
  + then: package each pilot as a runnable fixture (configs, transcripts, seed graph, expected headers) and wire futon3 checks so they reference those fixtures directly.
  + because: pilots only teach if they are reproducible; otherwise â€œjoy-as-empowered-actionâ€ is just lore.

! instantiated-by: Prototype 12 â€” Year-End Refactor & Packaging [ğŸŒ½/æœ« ğŸ™‡/ç”³]
  + context: By the end of the cycle you will have hardened v5, clarified modules, added exports, and integrated ingest, intents, and clients.
  + if: You want futon1 to be easy to install, run, and extend by others.
  + however: Accumulated tweaks across apps may fragment conventions (flags, env vars, logging, error reporting).
  + then: Do a pass to unify naming and configuration across apps, streamline build and test tasks, and publish a top-level futon1 README that explains the whole story in one place.
  + because: A coherent, well-packaged futon1 becomes the dependable base layer for FUTON2+ and for collaborators who were not present at inception.
