@arg ai4ci/sota-distinction
@title Distinguishing This Project from Current Mathematical AI SOTA
@sigils [ðŸš«/æœ¯ ðŸ§ /è¾¨]
@audience EPSRC reviewers
@tone plain-analytic
@allow-new-claims false

! conclusion:
  Understand that the project addresses a gap untouched by current mathematical
  AI systems: not solving problems, but exposing the reasoning cuesâ€”both
  exploratory and expositoryâ€”that humans rely on to understand mathematical work.

  + context: You are distinguishing AI4CI from current SOTA by focusing on explanatory cues rather than solution accuracy.
  + IF:
    State-of-the-art models such as ChatGPT or AlphaGeometry can often produce
    plausible solutions to textbook or competition-style problems, showing strong
    pattern-matching and symbolic fluency on polished inputs.

  + HOWEVER:
    These systems do not reliably check their own work, cannot explain their
    reasoning at the level needed for teaching, and fail to produce stable links
    between informal phrases and the specific steps or objects they reference.
    Their outputs also lack insight into how humans reconstruct proofs from
    published exposition.

  + THEN:
    Treat problem-solving capability as separate from explanatory capability, and
    focus the project on analysing how existing proofs communicate reasoning:
    what exploratory traces remain, what expository cues guide the reader, and
    whether these signals can be structurally annotated.

  + BECAUSE:
    Explanation and understanding depend on reconstructing intermediate
    motivations and relationships, not on producing correct final answers.
    Current SOTA models offer limited guidance here, leaving a gap that this
    pilot is uniquely positioned to explore.

  + NEXT-STEPS:
    next[Use AI only as a draft annotator for linking informal cues to symbolic steps.]
    next[Contrast human vs. model judgement on explanation quality.]
    next[Report where explanatory capability breaks down despite strong problem-solving.]
