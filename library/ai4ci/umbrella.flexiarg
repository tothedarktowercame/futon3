@flexiarg ai4ci/umbrella
@title AI4CI ‚Äî Human-Centred Future of AI
@sigils [üåê/‰πâ ‚ö°Ô∏è/ËÉΩ]
@audience EPSRC reviewers
@tone plain-analytic
@allow-new-claims false

! conclusion:
  Future AI that truly helps people learn demanding skills‚Äînot just solve tasks
  for them‚Äîrequires a structured account of the mid-level reasoning cues humans
  already use; this project treats the existence of a stable reasoning layer as
  an open hypothesis and tests whether today‚Äôs traces are recoverable enough to
  support dual-layer mathematics.

  + IF:
    Contemporary AI can already produce many correct answers on textbook-style
    problems but typically trains
    on polished outcomes, not on the reasoning processes that human learners rely
    on.

  + HOWEVER:
    Those processes do not appear as full interaction logs; they survive mainly
    in published exposition as hints, motivations, and tactic-describing phrases,
    and the project does not presume that these fragments already form a stable
    layer.  Stability, transferability, and utility remain empirical questions.

  + THEN:
    Treat expository traces as candidate signals, design a workflow that
    surfaces, validates, and evaluates them, and make the pass/fail criteria
    explicit: either we find a coherent mid-level layer or we publish the limits
    discovered by combining AI proposals, human review, and collective
    discussions.

  + BECAUSE:
    Only by making this tacit reasoning layer explicit can we provide the process
    structures future AI will need to train humans, build transparent teaching
    tools, and keep learners in the loop instead of simply emitting answers.

  + NEXT:
    next[Run the controlled mathematics pilot (dual-layer proofs + seminars) and report whether a recoverable reasoning layer exists that AI could train on.]
