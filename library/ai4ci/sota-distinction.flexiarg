@arg ai4ci/sota-distinction
@title Distinguishing This Project from Current Mathematical AI SOTA
@sigils [ðŸš«/æœ¯ ðŸ§ /è¾¨]
@audience EPSRC reviewers
@tone plain-analytic
@allow-new-claims false

! conclusion:
  Understand that the project addresses a gap untouched by current mathematical
  AI systems: not solving problems, but exposing the reasoning cuesâ€”both
  exploratory and expositoryâ€”that humans rely on to understand mathematical work.

  + IF:
    State-of-the-art models such as ChatGPT or AlphaGeometry can often produce
    plausible solutions to textbook or competition-style problems, showing strong
    pattern-matching and symbolic fluency on polished inputs.

  + HOWEVER:
    These systems do not reliably check their own work, cannot explain their
    reasoning at the level needed for teaching, and fail to produce stable links
    between informal phrases and the specific steps or objects they reference.
    Their outputs also lack insight into how humans reconstruct proofs from
    published exposition.

  + THEN:
    Treat problem-solving capability as separate from explanatory capability, and
    focus the project on analysing how existing proofs communicate reasoning:
    what exploratory traces remain, what expository cues guide the reader, and
    whether these signals can be structurally annotated.

  + BECAUSE:
    Explanation and understanding depend on reconstructing intermediate
    motivations and relationships, not on producing correct final answers.
    Current SOTA models offer limited guidance here, leaving a gap that this
    pilot is uniquely positioned to explore.

  + NEXT STEPS:
    Use AI only as a draft annotator for linking informal cues to symbolic steps;
    contrast human vs. model judgement; and report where explanatory capability
    breaks down even when problem-solving performance appears strong.
