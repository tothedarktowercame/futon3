@flexiarg ai4ci/ai-method-role
@title AI Incorporation ‚Äî Methodological Role
@sigils [üß∞/ÊúØ üîç/ÂØü]
@audience EPSRC reviewers
@tone plain-analytic
@allow-new-claims false
@factor Keen investigation (dhammavicaya)
@references [hdm/outline-first-detail-later hdm/human-machine-dialogue stack-coherence/ready-blocked-triage stack-coherence/staleness-scan]

! conclusion:
  Recognise that AI is treated as a hypothesis generator acting on a curated
  evaluation set: humans grade every suggestion against clarity, with MUSN
  logging where it succeeds, fails, or goes undefined.

  + IF:
    The HDM canon accepts outline-level reasoning before detail, but only when
    human stewards can interrogate the proposal.

  + HOWEVER:
    Without explicit logging of where AI suggestions were kept, edited, or
    discarded, we cannot tell whether the workflow learns anything.

  + THEN:
    Use existing checkpoints (math-tuned transformer NER/taggers and
    lightly-adapted rhetorical classifiers) for first-pass segmentation and
    paraphrase, with no autonomous mining beyond the curated proof set.  Route
    each AI suggestion through a ready/blocked ledger entry
    (`status[applies|blocked|drift]`) that is released with reproducibility
    notes, and treat staleness scans as alarms when drafts linger unreviewed.

  + BECAUSE:
    Methodological validity depends on showing that human judgement governs each
    explanation, not just aggregate scores, and on documenting the limits of the
    checkpoints we rely on.

  + STATUS:
    status[blocked]
    evidence[library/hdm/outline-first-detail-later.flexiarg:1]
    evidence[library/hdm/human-machine-dialogue.flexiarg:1]
    blocked-by[ai4ci/reviewer-roster]

  + NEXT:
    next[Publish reviewer rota + decision rubric so every AI-assisted explanation is triaged within 48h.]
    next[Wire review outcomes into stack-coherence/staleness-scan so unattended drafts raise alarms.]
